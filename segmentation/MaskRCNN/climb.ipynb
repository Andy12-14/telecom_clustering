{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "70d56af7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2.10.0+cu128\n"
                    ]
                }
            ],
            "source": [
                "import torch   \n",
                "print(torch.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "5430dc87",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "True\n",
                        "12.8\n"
                    ]
                }
            ],
            "source": [
                "# Vérifie si CUDA est disponible\n",
                "print(torch.cuda.is_available()) # Retourne True si CUDA est activé\n",
                "\n",
                "\n",
                "# Affiche la version de CUDA utilisée par PyTorch\n",
                "print(torch.version.cuda)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b9711b83",
            "metadata": {},
            "source": [
                "# 1. IMPORT DEPENDENCIES"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "d22ba370",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch.utils.data\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "from pycocotools.coco import COCO\n",
                "import torchvision.transforms as T\n",
                "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
                "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
                "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
                "from torchvision.transforms import functional as F\n",
                "import numpy as np\n",
                "import random"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "debug_path",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ClimbingDataset(torch.utils.data.Dataset):\n",
                "    def __init__(self, root, transforms=None):\n",
                "        self.root = root\n",
                "        self.transforms = transforms\n",
                "        \n",
                "        # Initialize COCO api\n",
                "        ann_file = os.path.join(root, \"_annotations.coco.json\")\n",
                "        if not os.path.exists(ann_file):\n",
                "            raise FileNotFoundError(f\"Annotation file not found at: {ann_file}\")\n",
                "\n",
                "        self.coco = COCO(ann_file)\n",
                "        # Sort initial keys\n",
                "        all_ids = list(sorted(self.coco.imgs.keys()))\n",
                "\n",
                "        # Filter out images that don't exist\n",
                "        self.ids = []\n",
                "        missing_count = 0\n",
                "        for img_id in all_ids:\n",
                "            img_metadata = self.coco.loadImgs(img_id)[0]\n",
                "            path = img_metadata['file_name']\n",
                "            img_path = os.path.join(self.root, path)\n",
                "            if os.path.exists(img_path):\n",
                "                self.ids.append(img_id)\n",
                "            else:\n",
                "                missing_count += 1\n",
                "        \n",
                "        print(f\"Total potential images: {len(all_ids)}\")\n",
                "        print(f\"Missing images filtered out: {missing_count}\")\n",
                "        print(f\"Valid images loaded: {len(self.ids)}\")\n",
                "\n",
                "    def __getitem__(self, index):\n",
                "        # Load Image\n",
                "        coco = self.coco\n",
                "        img_id = self.ids[index]\n",
                "        img_metadata = coco.loadImgs(img_id)[0]\n",
                "        path = img_metadata['file_name']\n",
                "        img_path = os.path.join(self.root, path)\n",
                "        \n",
                "        # Image is guaranteed to exist now due to init filtering\n",
                "        img = Image.open(img_path).convert(\"RGB\")\n",
                "\n",
                "        # Load Annotations\n",
                "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
                "        anns = coco.loadAnns(ann_ids)\n",
                "\n",
                "        num_objs = len(anns)\n",
                "        boxes = []\n",
                "        masks = []\n",
                "        labels = []\n",
                "\n",
                "        for ann in anns:\n",
                "            xmin, ymin, w, h = ann['bbox']\n",
                "            boxes.append([xmin, ymin, xmin + w, ymin + h])\n",
                "            labels.append(ann['category_id'])\n",
                "            masks.append(coco.annToMask(ann))\n",
                "\n",
                "        # Convert to Tensors\n",
                "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
                "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
                "        if num_objs > 0:\n",
                "            masks = np.array(masks)\n",
                "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
                "        else:\n",
                "            masks = torch.zeros((0, img_metadata['height'], img_metadata['width']), dtype=torch.uint8)\n",
                "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
                "\n",
                "        image_id = torch.tensor([img_id])\n",
                "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
                "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
                "\n",
                "        target = {}\n",
                "        target[\"boxes\"] = boxes\n",
                "        target[\"labels\"] = labels\n",
                "        target[\"masks\"] = masks\n",
                "        target[\"image_id\"] = image_id\n",
                "        target[\"area\"] = area\n",
                "        target[\"iscrowd\"] = iscrowd\n",
                "\n",
                "        if self.transforms is not None:\n",
                "            img = self.transforms(img)\n",
                "\n",
                "        return img, target\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.ids)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dataset_section",
            "metadata": {},
            "source": [
                "# 2. DEFINE DATASET CLASS\n",
                "We implement a custom Dataset that reads COCO annotations and converts polygons to masks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "dataset_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Helper Functions ---\n",
                "def get_transform(train):\n",
                "    custom_transforms = []\n",
                "    custom_transforms.append(T.ToTensor())\n",
                "    return T.Compose(custom_transforms)\n",
                "\n",
                "def collate_fn(batch):\n",
                "    return tuple(zip(*batch))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "8a693701",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking train set...\n",
                        "Total entries in JSON: 117\n",
                        "Valid images: 114\n",
                        "Missing images: 3\n",
                        "Checking test set...\n",
                        "Total entries in JSON: 1\n",
                        "Valid images: 1\n",
                        "Missing images: 0\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import json\n",
                "\n",
                "def check_dataset(split='train'):\n",
                "    base_dir = r\"C:\\Users\\silue\\Documents\\telecom_clustering\\segmentation\\MaskRCNN\"\n",
                "    dataset_dir = os.path.join(base_dir, \"Hold Detector.v2i.coco-segmentation\", split)\n",
                "    ann_file = os.path.join(dataset_dir, \"_annotations.coco.json\")\n",
                "    \n",
                "    if not os.path.exists(ann_file):\n",
                "        print(f\"Annotation file not found: {ann_file}\")\n",
                "        return\n",
                "\n",
                "    with open(ann_file, 'r') as f:\n",
                "        data = json.load(f)\n",
                "    \n",
                "    total_images = len(data['images'])\n",
                "    missing_count = 0\n",
                "    valid_count = 0\n",
                "    \n",
                "    print(f\"Checking {split} set...\")\n",
                "    print(f\"Total entries in JSON: {total_images}\")\n",
                "    \n",
                "    for img in data['images']:\n",
                "        fname = img['file_name']\n",
                "        img_path = os.path.join(dataset_dir, fname)\n",
                "        if not os.path.exists(img_path):\n",
                "            missing_count += 1\n",
                "            # print(f\"Missing: {fname}\")\n",
                "        else:\n",
                "            valid_count += 1\n",
                "            \n",
                "    print(f\"Valid images: {valid_count}\")\n",
                "    print(f\"Missing images: {missing_count}\")\n",
                "\n",
                "check_dataset('train')\n",
                "check_dataset('test')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model_config_section",
            "metadata": {},
            "source": [
                "# 3. CONFIGURE MODEL\n",
                "We fine-tune a pre-trained Mask R-CNN model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "model_config_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_model_instance_segmentation(num_classes):\n",
                "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
                "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
                "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
                "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
                "    hidden_layer = 256\n",
                "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training_section",
            "metadata": {},
            "source": [
                "# 4. TRAINING LOOP"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "training_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):\n",
                "    model.train()\n",
                "    for i, (images, targets) in enumerate(data_loader):\n",
                "        images = list(image.to(device) for image in images)\n",
                "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
                "\n",
                "        loss_dict = model(images, targets)\n",
                "        losses = sum(loss for loss in loss_dict.values())\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        losses.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        if i % print_freq == 0:\n",
                "            print(f\"Epoch: {epoch}, Batch: {i}, Loss: {losses.item()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "viz_section",
            "metadata": {},
            "source": [
                "# 5. VISUALIZATION\n",
                "Visualize predictions on a test image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "viz_code",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking the device\n",
                        "\n",
                        "The device use is cuda.\n",
                        "\n",
                        "Initializing datasets...\n",
                        "loading annotations into memory...\n",
                        "Done (t=0.14s)\n",
                        "creating index...\n",
                        "index created!\n",
                        "Total potential images: 117\n",
                        "Missing images filtered out: 3\n",
                        "Valid images loaded: 114\n",
                        "loading annotations into memory...\n",
                        "Done (t=0.00s)\n",
                        "creating index...\n",
                        "index created!\n",
                        "Total potential images: 1\n",
                        "Missing images filtered out: 0\n",
                        "Valid images loaded: 1\n",
                        "\n",
                        "Loading model...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\silue\\Documents\\telecom_clustering\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
                        "  warnings.warn(\n",
                        "c:\\Users\\silue\\Documents\\telecom_clustering\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
                        "  warnings.warn(msg)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Starting training...\n",
                        "Epoch: 0, Batch: 0, Loss: 11.459665298461914\n",
                        "Epoch: 0, Batch: 10, Loss: 2.1420774459838867\n",
                        "Epoch: 0, Batch: 20, Loss: 2.339132785797119\n",
                        "Epoch: 0, Batch: 30, Loss: 2.274705410003662\n",
                        "Epoch: 0, Batch: 40, Loss: 2.3343803882598877\n",
                        "Epoch: 0, Batch: 50, Loss: 1.681612491607666\n",
                        "Epoch: 1, Batch: 0, Loss: 1.3708373308181763\n",
                        "Epoch: 1, Batch: 10, Loss: 1.424757957458496\n",
                        "Epoch: 1, Batch: 20, Loss: 1.425351619720459\n",
                        "Epoch: 1, Batch: 30, Loss: 1.818040370941162\n",
                        "Epoch: 1, Batch: 40, Loss: 1.4922174215316772\n",
                        "Epoch: 1, Batch: 50, Loss: 1.7108650207519531\n",
                        "Epoch: 2, Batch: 0, Loss: 1.5467612743377686\n",
                        "Epoch: 2, Batch: 10, Loss: 1.1366422176361084\n",
                        "Epoch: 2, Batch: 20, Loss: 1.0448704957962036\n",
                        "Epoch: 2, Batch: 30, Loss: 1.365187406539917\n",
                        "Epoch: 2, Batch: 40, Loss: 1.1291300058364868\n",
                        "Epoch: 2, Batch: 50, Loss: 1.6019426584243774\n",
                        "Epoch: 3, Batch: 0, Loss: 1.4601249694824219\n",
                        "Epoch: 3, Batch: 10, Loss: 1.2617735862731934\n",
                        "Epoch: 3, Batch: 20, Loss: 1.4798154830932617\n",
                        "Epoch: 3, Batch: 30, Loss: 1.22205650806427\n",
                        "Epoch: 3, Batch: 40, Loss: 1.4161577224731445\n",
                        "Epoch: 3, Batch: 50, Loss: 1.2453224658966064\n",
                        "Epoch: 4, Batch: 0, Loss: 1.3636596202850342\n",
                        "Epoch: 4, Batch: 10, Loss: 0.936896562576294\n",
                        "Epoch: 4, Batch: 20, Loss: 1.0081439018249512\n",
                        "Epoch: 4, Batch: 30, Loss: 1.3980567455291748\n",
                        "Epoch: 4, Batch: 40, Loss: 0.9054288268089294\n",
                        "Epoch: 4, Batch: 50, Loss: 0.8684053421020508\n",
                        "Training Complete!\n"
                    ]
                }
            ],
            "source": [
                "dataset_path = 'Hold Detector.v2i.coco-segmentation'\n",
                "train_path = os.path.join(dataset_path, 'train')\n",
                "test_path = os.path.join(dataset_path, 'test')\n",
                "\n",
                "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "num_classes = 2 # background + hold\n",
                "\n",
                "print(\"Checking the device\")\n",
                "print()\n",
                "print(f\"The device use is {device}.\")\n",
                "print()\n",
                "print(f\"Initializing datasets...\")\n",
                "# Initialize datasets with the Robust ClimbingDataset\n",
                "dataset_train = ClimbingDataset(train_path, get_transform(train=True))\n",
                "dataset_test = ClimbingDataset(test_path, get_transform(train=False))\n",
                "\n",
                "data_loader = torch.utils.data.DataLoader(\n",
                "    dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
                ")\n",
                "\n",
                "data_loader_test = torch.utils.data.DataLoader(\n",
                "    dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn\n",
                ")\n",
                "print()\n",
                "print(\"Loading model...\")\n",
                "model = get_model_instance_segmentation(num_classes)\n",
                "model.to(device)\n",
                "\n",
                "params = [p for p in model.parameters() if p.requires_grad]\n",
                "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
                "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
                "\n",
                "# Train for 1 Epoch\n",
                "num_epochs = 5\n",
                "print()\n",
                "print(\"Starting training...\")\n",
                "for epoch in range(num_epochs):\n",
                "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
                "    lr_scheduler.step()\n",
                "\n",
                "print(\"Training Complete!\")\n",
                "torch.save(model.state_dict(), f\"climbing_model_epoch{num_epochs}.pth\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ebac06b2",
            "metadata": {},
            "outputs": [
                {
                    "ename": "IndentationError",
                    "evalue": "unexpected indent (1841942721.py, line 2)",
                    "output_type": "error",
                    "traceback": [
                        "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif not os.path.exists(model_path):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
                    ]
                }
            ],
            "source": [
                "def main():\n",
                "    # --- Setup ---\n",
                "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "    print(f\"Using device: {device}\")\n",
                "\n",
                "    # --- Load Model ---\n",
                "    num_classes = 2 # background + hold\n",
                "    model = get_model_instance_segmentation(num_classes)\n",
                "\n",
                "    model_path = 'climbing_model.pth'\n",
                "    if not os.path.exists(model_path):\n",
                "        print(f\"Error: Model file '{model_path}' not found.\")\n",
                "        return\n",
                "print(f\"Loading model from {model_path}...\")\n",
                "# Load state dict\n",
                "state_dict = torch.load(model_path, map_location=device)\n",
                "model.load_state_dict(state_dict)\n",
                "model.to(device)\n",
                "model.eval()\n",
                "\n",
                "# --- Load Test Image ---\n",
                "# Path to the specific test image found earlier\n",
                "dataset_path = 'Hold Detector.v2i.coco-segmentation'\n",
                "test_dir = os.path.join(dataset_path, 'test')\n",
                "\n",
                "# Try to find a valid image\n",
                "img_name = '43d78bf0874b7b6def76e7d846ec6fc4_jpg.rf.8463ddc27821845eeff514149e782f7b.jpg'\n",
                "img_path = os.path.join(test_dir, img_name)\n",
                "\n",
                "if not os.path.exists(img_path):\n",
                "    print(f\"Specific image not found at {img_path}, looking for any jpg in {test_dir}...\")\n",
                "    if os.path.exists(test_dir):\n",
                "        for f in os.listdir(test_dir):\n",
                "            if f.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
                "                img_path = os.path.join(test_dir, f)\n",
                "                break\n",
                "\n",
                "if not os.path.exists(img_path):\n",
                "    print(\"No test image found.\")\n",
                "    return\n",
                "\n",
                "print(f\"Using image: {img_path}\")\n",
                "\n",
                "image = Image.open(img_path).convert(\"RGB\")\n",
                "image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
                "\n",
                "# --- Inference ---\n",
                "print(\"Running inference...\")\n",
                "with torch.no_grad():\n",
                "    prediction = model(image_tensor)\n",
                "\n",
                "# --- Visualization ---\n",
                "print(\"Visualizing results...\")\n",
                "\n",
                "img_np = np.array(image)\n",
                "\n",
                "# Create figure and axes\n",
                "fig, ax = plt.subplots(1, figsize=(12, 9))\n",
                "ax.imshow(img_np)\n",
                "\n",
                "pred_score_threshold = 0.5\n",
                "pred_boxes = prediction[0]['boxes'].cpu().numpy()\n",
                "pred_masks = prediction[0]['masks'].cpu().numpy()\n",
                "pred_scores = prediction[0]['scores'].cpu().numpy()\n",
                "\n",
                "# Helper function for random colors\n",
                "def random_color():\n",
                "    return (random.random(), random.random(), random.random())\n",
                "\n",
                "num_instances = 0\n",
                "for i in range(len(pred_masks)):\n",
                "    if pred_scores[i] > pred_score_threshold:\n",
                "        num_instances += 1\n",
                "        mask = pred_masks[i, 0]\n",
                "        mask = (mask > 0.5) # Boolean mask\n",
                "        \n",
                "        if mask.sum() > 0:\n",
                "            color = random_color()\n",
                "            \n",
                "            # Apply mask overlay\n",
                "            # Create an RGBA image for the mask\n",
                "            mask_rgba = np.zeros((mask.shape[0], mask.shape[1], 4))\n",
                "            mask_rgba[mask, 0:3] = color\n",
                "            mask_rgba[mask, 3] = 0.5 # Alpha\n",
                "            \n",
                "            ax.imshow(mask_rgba)\n",
                "            \n",
                "            # Draw bounding box\n",
                "            box = pred_boxes[i]\n",
                "            x_min, y_min, x_max, y_max = box\n",
                "            rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
                "                                    fill=False, edgecolor=color, linewidth=2)\n",
                "            ax.add_patch(rect)\n",
                "            ax.text(x_min, y_min - 5, f'{pred_scores[i]:.2f}', color=color, fontsize=10, weight='bold')\n",
                "\n",
                "plt.axis('off')\n",
                "plt.title(f\"Detected {num_instances} holds (Threshold: {pred_score_threshold})\")\n",
                "\n",
                "output_file = 'prediction_result_epoch5.png'\n",
                "plt.savefig(output_file, bbox_inches='tight')\n",
                "print(f\"Result saved to {output_file}\")\n",
                "plt.show()\n",
                "\n",
                "plt.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ae28294e",
            "metadata": {},
            "source": [
                "#  ****"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2135b31a",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f3b2ec28",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
