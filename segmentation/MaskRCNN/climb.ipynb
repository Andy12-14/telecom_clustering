{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "70d56af7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2.10.0+cu128\n"
                    ]
                }
            ],
            "source": [
                "import torch   \n",
                "print(torch.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "5430dc87",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "True\n",
                        "12.8\n"
                    ]
                }
            ],
            "source": [
                "# Vérifie si CUDA est disponible\n",
                "print(torch.cuda.is_available()) # Retourne True si CUDA est activé\n",
                "\n",
                "\n",
                "# Affiche la version de CUDA utilisée par PyTorch\n",
                "print(torch.version.cuda)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b9711b83",
            "metadata": {},
            "source": [
                "# 1. IMPORT DEPENDENCIES"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "d22ba370",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "from PIL import Image\n",
                "import torchvision\n",
                "from torchvision import transforms as T\n",
                "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
                "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
                "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from pycocotools.coco import COCO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "debug_path",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Current Working Directory: c:\\Users\\silue\\Documents\\telecom_clustering\\segmentation\\MaskRCNN\n",
                        "Files in CWD: ['climb.ipynb', 'climbing_model.pth', 'climb_epoch1.ipynb', 'epoch1.py', 'Hold Detector.v2i.coco-segmentation', 'perf.txt', 'requirements.txt']\n",
                        "Dataset folder 'Hold Detector.v2i.coco-segmentation' FOUND.\n",
                        "Using Train Path: Hold Detector.v2i.coco-segmentation\\train\n"
                    ]
                }
            ],
            "source": [
                "import os \n",
                "\n",
                "# DEBUG PATHS\\n\n",
                "print(\"Current Working Directory:\", os.getcwd())\n",
                "print(\"Files in CWD:\", os.listdir(os.getcwd()))\n",
                "\n",
                "dataset_path = 'Hold Detector.v2i.coco-segmentation'\n",
                "if os.path.exists(dataset_path):\n",
                "    print(f\"Dataset folder '{dataset_path}' FOUND.\")\n",
                "else:\n",
                "    print(f\"Dataset folder '{dataset_path}' NOT FOUND in CWD.\")\n",
                "    # Attempt to look in common parent folders if not found\n",
                "    possible_paths = ['MaskRCNN/Hold Detector.v2i.coco-segmentation', '../MaskRCNN/Hold Detector.v2i.coco-segmentation']\n",
                "    for p in possible_paths:\n",
                "        if os.path.exists(p):\n",
                "            print(f\"Found at alternative path: {p}\")\n",
                "            dataset_path = p\n",
                "            break\n",
                "\n",
                "train_path = os.path.join(dataset_path, 'train')\n",
                "test_path = os.path.join(dataset_path, 'test')\n",
                "print(f\"Using Train Path: {train_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dataset_section",
            "metadata": {},
            "source": [
                "# 2. DEFINE DATASET CLASS\n",
                "We implement a custom Dataset that reads COCO annotations and converts polygons to masks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "dataset_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ClimbingDataset(Dataset):\n",
                "    def __init__(self, root, transforms=None):\n",
                "        self.root = root\n",
                "        self.transforms = transforms\n",
                "        \n",
                "        # Initialize COCO api\n",
                "        ann_file = os.path.join(root, \"_annotations.coco.json\")\n",
                "        if not os.path.exists(ann_file):\n",
                "            raise FileNotFoundError(f\"Annotation file not found at: {ann_file}\")\n",
                "\n",
                "        self.coco = COCO(ann_file)\n",
                "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
                "\n",
                "    def __getitem__(self, index):\n",
                "        # Load Image\n",
                "        coco = self.coco\n",
                "        img_id = self.ids[index]\n",
                "        img_metadata = coco.loadImgs(img_id)[0]\n",
                "        path = img_metadata['file_name']\n",
                "        img_path = os.path.join(self.root, path)\n",
                "        \n",
                "        if not os.path.exists(img_path):\n",
                "             # Fallback if image is not right in root (COCO sometimes has flat structure)\n",
                "             print(f\"Warning: Image not found at {img_path}\")\n",
                "\n",
                "        img = Image.open(img_path).convert(\"RGB\")\n",
                "\n",
                "        # Load Annotations\n",
                "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
                "        anns = coco.loadAnns(ann_ids)\n",
                "\n",
                "        num_objs = len(anns)\n",
                "        boxes = []\n",
                "        masks = []\n",
                "        labels = []\n",
                "\n",
                "        for ann in anns:\n",
                "            xmin, ymin, w, h = ann['bbox']\n",
                "            boxes.append([xmin, ymin, xmin + w, ymin + h])\n",
                "            labels.append(ann['category_id'])\n",
                "            masks.append(coco.annToMask(ann))\n",
                "\n",
                "        # Convert to Tensors\n",
                "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
                "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
                "        # Masks need to be uint8 for proper tensor conversion later\n",
                "        if num_objs > 0:\n",
                "            masks = np.array(masks)\n",
                "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
                "        else:\n",
                "            # Handle empty image case (optional, usually training sets have objs)\n",
                "            masks = torch.zeros((0, img_metadata['height'], img_metadata['width']), dtype=torch.uint8)\n",
                "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
                "\n",
                "        image_id = torch.tensor([img_id])\n",
                "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
                "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
                "\n",
                "        target = {}\n",
                "        target[\"boxes\"] = boxes\n",
                "        target[\"labels\"] = labels\n",
                "        target[\"masks\"] = masks\n",
                "        target[\"image_id\"] = image_id\n",
                "        target[\"area\"] = area\n",
                "        target[\"iscrowd\"] = iscrowd\n",
                "\n",
                "        if self.transforms is not None:\n",
                "            img = self.transforms(img)\n",
                "\n",
                "        return img, target\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.ids)\n",
                "    \n",
                "def get_transform(train):\n",
                "    custom_transforms = []\n",
                "    custom_transforms.append(T.ToTensor())\n",
                "    # We can add augmentation here if needed\n",
                "    return T.Compose(custom_transforms)\n",
                "\n",
                "def collate_fn(batch):\n",
                "    return tuple(zip(*batch))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "8a693701",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking train set...\n",
                        "Total entries in JSON: 117\n",
                        "Valid images: 114\n",
                        "Missing images: 3\n",
                        "Checking test set...\n",
                        "Total entries in JSON: 1\n",
                        "Valid images: 1\n",
                        "Missing images: 0\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import json\n",
                "\n",
                "def check_dataset(split='train'):\n",
                "    base_dir = r\"C:\\Users\\silue\\Documents\\telecom_clustering\\segmentation\\MaskRCNN\"\n",
                "    dataset_dir = os.path.join(base_dir, \"Hold Detector.v2i.coco-segmentation\", split)\n",
                "    ann_file = os.path.join(dataset_dir, \"_annotations.coco.json\")\n",
                "    \n",
                "    if not os.path.exists(ann_file):\n",
                "        print(f\"Annotation file not found: {ann_file}\")\n",
                "        return\n",
                "\n",
                "    with open(ann_file, 'r') as f:\n",
                "        data = json.load(f)\n",
                "    \n",
                "    total_images = len(data['images'])\n",
                "    missing_count = 0\n",
                "    valid_count = 0\n",
                "    \n",
                "    print(f\"Checking {split} set...\")\n",
                "    print(f\"Total entries in JSON: {total_images}\")\n",
                "    \n",
                "    for img in data['images']:\n",
                "        fname = img['file_name']\n",
                "        img_path = os.path.join(dataset_dir, fname)\n",
                "        if not os.path.exists(img_path):\n",
                "            missing_count += 1\n",
                "            # print(f\"Missing: {fname}\")\n",
                "        else:\n",
                "            valid_count += 1\n",
                "            \n",
                "    print(f\"Valid images: {valid_count}\")\n",
                "    print(f\"Missing images: {missing_count}\")\n",
                "\n",
                "check_dataset('train')\n",
                "check_dataset('test')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model_config_section",
            "metadata": {},
            "source": [
                "# 3. CONFIGURE MODEL\n",
                "We fine-tune a pre-trained Mask R-CNN model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "model_config_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_model_instance_segmentation(num_classes):\n",
                "    # load an instance segmentation model pre-trained on COCO\n",
                "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
                "\n",
                "    # get number of input features for the classifier\n",
                "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
                "    \n",
                "    # replace the pre-trained head with a new one\n",
                "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
                "\n",
                "    # now get the number of input features for the mask classifier\n",
                "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
                "    hidden_layer = 256\n",
                "    \n",
                "    # and replace the mask predictor with a new one\n",
                "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
                "\n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training_section",
            "metadata": {},
            "source": [
                "# 4. TRAINING LOOP"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "training_code",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initializing datasets with train path: Hold Detector.v2i.coco-segmentation\\train and test path: Hold Detector.v2i.coco-segmentation\\test\n",
                        "loading annotations into memory...\n",
                        "Done (t=0.12s)\n",
                        "creating index...\n",
                        "index created!\n",
                        "loading annotations into memory...\n",
                        "Done (t=0.00s)\n",
                        "creating index...\n",
                        "index created!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\silue\\Documents\\telecom_clustering\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
                        "  warnings.warn(\n",
                        "c:\\Users\\silue\\Documents\\telecom_clustering\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
                        "  warnings.warn(msg)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Warning: Image not found at Hold Detector.v2i.coco-segmentation\\train\\climbing-hold-climbing-bouldering-sport-climbing-adventure-rock-climbing-climbing-shoe-free-climbing-recreation-rock-climbing-equipment-wall-individual-sports-competition-leisure-competition-event-plastic-concrete-rock-back-play-1621676_jpg.rf.cc9a2c229ee40a15e1e73a56c909b2c6.jpg\n"
                    ]
                },
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: 'Hold Detector.v2i.coco-segmentation\\\\train\\\\climbing-hold-climbing-bouldering-sport-climbing-adventure-rock-climbing-climbing-shoe-free-climbing-recreation-rock-climbing-equipment-wall-individual-sports-competition-leisure-competition-event-plastic-concrete-rock-back-play-1621676_jpg.rf.cc9a2c229ee40a15e1e73a56c909b2c6.jpg'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     55\u001b[39m num_epochs = \u001b[32m1\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     lr_scheduler.step()\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_one_epoch\u001b[39m(model, optimizer, data_loader, device, epoch, print_freq=\u001b[32m10\u001b[39m):\n\u001b[32m     39\u001b[39m     model.train()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\silue\\Documents\\telecom_clustering\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\silue\\Documents\\telecom_clustering\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:801\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    800\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    803\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\silue\\Documents\\telecom_clustering\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mClimbingDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(img_path):\n\u001b[32m     23\u001b[39m      \u001b[38;5;66;03m# Fallback if image is not right in root (COCO sometimes has flat structure)\u001b[39;00m\n\u001b[32m     24\u001b[39m      \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Image not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Load Annotations\u001b[39;00m\n\u001b[32m     29\u001b[39m ann_ids = coco.getAnnIds(imgIds=img_id)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\silue\\Documents\\telecom_clustering\\.venv\\Lib\\site-packages\\PIL\\Image.py:3512\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3511\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3512\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3513\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3514\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Hold Detector.v2i.coco-segmentation\\\\train\\\\climbing-hold-climbing-bouldering-sport-climbing-adventure-rock-climbing-climbing-shoe-free-climbing-recreation-rock-climbing-equipment-wall-individual-sports-competition-leisure-competition-event-plastic-concrete-rock-back-play-1621676_jpg.rf.cc9a2c229ee40a15e1e73a56c909b2c6.jpg'"
                    ]
                }
            ],
            "source": [
                "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "\n",
                "# 2 classes: background + hold\n",
                "num_classes = 2\n",
                "\n",
                "# Use our robust paths logic from above variable\n",
                "print(f\"Initializing datasets with train path: {train_path} and test path: {test_path}\")\n",
                "\n",
                "# Use our dataset and defined transformations\n",
                "dataset_train = ClimbingDataset(train_path, get_transform(train=True))\n",
                "dataset_test = ClimbingDataset(test_path, get_transform(train=False))\n",
                "\n",
                "# Define training and validation data loaders\n",
                "data_loader = DataLoader(\n",
                "    dataset_train, batch_size=2, shuffle=True,  # Low batch size for Mask RCNN usually safe\n",
                "    collate_fn=collate_fn\n",
                ")\n",
                "\n",
                "data_loader_test = DataLoader(\n",
                "    dataset_test, batch_size=1, shuffle=False,\n",
                "    collate_fn=collate_fn\n",
                ")\n",
                "\n",
                "# Get the model using our helper function\n",
                "model = get_model_instance_segmentation(num_classes)\n",
                "\n",
                "# Move model to the right device\n",
                "model.to(device)\n",
                "\n",
                "# Construct an optimizer\n",
                "params = [p for p in model.parameters() if p.requires_grad]\n",
                "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
                "\n",
                "# Learning rate scheduler\n",
                "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
                "\n",
                "# Training function\n",
                "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):\n",
                "    model.train()\n",
                "    for i, (images, targets) in enumerate(data_loader):\n",
                "        images = list(image.to(device) for image in images)\n",
                "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
                "\n",
                "        loss_dict = model(images, targets)\n",
                "        losses = sum(loss for loss in loss_dict.values())\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        losses.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        if i % print_freq == 0:\n",
                "            print(f\"Epoch: {epoch}, Batch: {i}, Loss: {losses.item()}\")\n",
                "\n",
                "# Train for 1 Epoch (for demonstration)\n",
                "num_epochs = 1\n",
                "for epoch in range(num_epochs):\n",
                "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
                "    lr_scheduler.step()\n",
                "\n",
                "print(\"Training Complete!\")\n",
                "\n",
                "# Save Model\n",
                "torch.save(model.state_dict(), \"climbing_model.pth\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "viz_section",
            "metadata": {},
            "source": [
                "# 5. VISUALIZATION\n",
                "Visualize predictions on a test image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "viz_code",
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'model' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-3259810406.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
                    ]
                }
            ],
            "source": [
                "model.eval()\n",
                "img, _ = dataset_test[0]\n",
                "\n",
                "with torch.no_grad():\n",
                "    prediction = model([img.to(device)])\n",
                "\n",
                "print(\"Number of detected holds:\", len(prediction[0]['masks']))\n",
                "\n",
                "# Visualize\n",
                "img_np = img.mul(255).permute(1, 2, 0).byte().numpy()\n",
                "plt.figure(figsize=(12, 12))\n",
                "plt.imshow(img_np)\n",
                "\n",
                "# Overlay masks (simple logic)\n",
                "for i in range(len(prediction[0]['masks'])):\n",
                "    mask = prediction[0]['masks'][i, 0].mul(255).byte().cpu().numpy()\n",
                "    score = prediction[0]['scores'][i].item()\n",
                "    if score > 0.5: # Filter low confidence\n",
                "        plt.imshow(mask, cmap='jet', alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ebac06b2",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2135b31a",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f3b2ec28",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
